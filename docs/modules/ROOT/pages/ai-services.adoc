= Designing AI services

include::./includes/attributes.adoc[]

An _AI Service_ employs a declarative approach to define interactions with the LLM, serving as the pivotal interaction point between your application and the LLM. It operates as an intermediary, known as an https://learn.microsoft.com/en-us/azure/architecture/patterns/ambassador[_ambassador_].

== Purpose

The _AI Service_ serves as the core connection point between your application and the LLM. It abstracts the LLM specifics, encapsulating and declaring all interactions within a singular interface.

== Leveraging @RegisterAiService

The `@RegisterAiService` annotation is pivotal for registering an _AI Service_, placed on an interface:

[source,java]
----
@RegisterAiService
public interface MyAiService {
    // methods.
}
----

Once registered, you can inject the _AI Service_ into your application:

[source,java]
----
@Inject MyAiService service;
----

[IMPORTANT]
====
The beans created by `@RegisterAiService` are `@RequestScoped` by default. The reason for this is that it enables removing chat <<memory>> objects.
This is a good default when a service is used during when handling an HTTP request, but it's inappropriate in CLIs or in WebSockets (currently, but may change in the future).
For example when using a service in a CLI, it makes sense to have the service be `@ApplicationScoped` and the extension allows this simply if the service is annotated with `@ApplicationScoped`.
====

== AI method declaration

Within the interface annotated with `@RegisterAiService`, you model interactions with the LLM using _methods_. These methods accept parameters and are annotated with `@SystemMessage` and `@UserMessage` to define instructions directed to the LLM:

[source,java]
----
@SystemMessage("You are a professional poet.")
@UserMessage("""
    Write a poem about {topic}. The poem should be {lines} lines long. Then send this poem by email.
""")
String writeAPoem(String topic, int lines);
----

[#_system_message]
=== System Message

The `@SystemMessage` annotation defines the scope and initial instructions, serving as the first message sent to the LLM. It delineates the AI service's role in the interaction:

[source,java]
----
@SystemMessage("""
    You are working for a bank, processing reviews about financial products. Triage reviews into positive and negative ones, responding with a JSON document.
    """
)
----

=== User Message (Prompt)

The `@UserMessage` annotation defines primary instructions dispatched to the LLM. It typically encompasses requests and the expected response format:

[source,java]
----
@UserMessage("""
    Your task is to process the review delimited by ---.
    Apply a sentiment analysis to the review to determine if it is positive or negative, considering various languages.

    For example:
    - "I love your bank, you are the best!" is a 'POSITIVE' review
    - "J'adore votre banque" is a 'POSITIVE' review
    - "I hate your bank, you are the worst!" is a 'NEGATIVE' review

    Respond with a JSON document containing:
    - the 'evaluation' key set to 'POSITIVE' if the review is positive, 'NEGATIVE' otherwise
    - the 'message' key set to a message thanking or apologizing to the customer. These messages must be polite and match the review's language.

    ---
    {review}
    ---
""")
TriagedReview triage(String review);
----

=== Parameter Passing and Referencing

AI methods can take parameters referenced in system and user messages using the `\{parameter}` syntax:

[source,java]
----
@SystemMessage("You are a professional poet")
@UserMessage("""
    Write a poem about {topic}. The poem should be {lines} lines long. Then send this poem by email.
""")
String writeAPoem(String topic, int lines);
----

[#_ai_method_return_type]
=== AI Method Return Type

If the _prompt_ defines the JSON response format precisely, you can map the response directly to an object:

[source,java]
----
// ... See above for the prompt
TriagedReview triage(String review);
----

In this instance, Quarkus automatically creates an instance of `TriagedReview` from the LLM's JSON response.

=== Receiving User Message as a Parameter

For situations requiring the user message to be passed as a parameter, you can use the `@UserMessage` annotation on a parameter. Exercise caution with this feature, especially when the AI has access to _tools_:

[source,java]
----
String chat(@UserMessage String userMessage);
----

The annotated parameter should be of type `String`.

=== Receiving MemoryId as a Parameter

The _memory_ encompasses the cumulative context of the interaction with the LLM. To manage statelessness in LLMs, the complete context must be exchanged between the LLM and the AI service.

Hence, the AI Service can store the latest messages in a _memory_, often termed _context_. The `@MemoryId` annotation enables referencing the memory for a specific user in the AI method:

[source,java]
----
String chat(@MemoryId int memoryId, @UserMessage String userMessage);
----

We'll explore an alternative approach to avoid manual memory handling in the <<memory>> section.

== Configuring the Chat Language Model

While LLMs are the base AI models, the chat language model builds upon them, enabling chat-like interactions. If you have a single chat language model, no specific configuration is required.

However, with multiple models like OpenAI and Hugging Faces, you need to specify which one to use:

[source,java]
----
@RegisterAiService(
    chatLanguageModelSupplier = MyChatModelSupplier.class
)
----

The `MyChatModelSupplier` class should implement the `Supplier<ChatLanguageModel>` interface:

[source,java]
----
package io.quarkiverse.langchain4j.sample;

import dev.langchain4j.model.chat.ChatLanguageModel;
import dev.langchain4j.model.openai.OpenAiChatModel;

import java.util.function.Supplier;

public class MyChatModelSupplier implements Supplier<ChatLanguageModel> {
    @Override
    public ChatLanguageModel get() {
        return OpenAiChatModel.builder()
                .apiKey("...")
                .build();
    }
}
----

[#memory]
== Configuring the Context (Memory)

As LLMs are stateless, the memory — comprising the interaction context — must be exchanged each time. To prevent storing excessive messages, it's crucial to evict older messages.

The `chatMemoryProviderSupplier` attribute of the `@RegisterAiService` annotation enables configuring the memory provider. The default value of this annotation is `RegisterAiService.BeanIfExistsChatMemoryProviderSupplier.class`
which means that the `AiService` will use whatever `ChatMemoryProvider` bean is configured by the application, while falling back to no memory if no such bean exists.
An example of such a bean is:

[source,java]
----
include::{examples-dir}/io/quarkiverse/langchain4j/samples/ChatMemoryProviderBean.java[]
----

Additionally, it is important to configure a `ChatMemoryStore` which is responsible for actually storing the chat memory.
The following example shows how an application wide `InMemoryChatMemoryStore` can be used for this.

[source,java]
----
include::{examples-dir}/io/quarkiverse/langchain4j/samples/ChatMemoryStoreProducer.java[]
----

Notice that with this example, conversations are not deleted and thus accumulate over time.

NOTE: It is recommended to have your chat memory beans implement `RemovableChatMemoryProvider` because the objects used as memory IDs are removed from the memory when the service goes out of scope.

Users can provide their own custom `ChatMemoryProvider` for use in the AiService by implementing `Supplier<ChatMemoryProvider>`, such as:

[source,java]
----
include::{examples-dir}/io/quarkiverse/langchain4j/samples/MySmallMemoryProvider.java[]
----

and configuring the AiService as so:

[source,java]
----
@RegisterAiService(
    chatMemoryProviderSupplier = MySmallMemoryProvider.class)
----

TIP: For non-memory-reliant LLM interactions, you may skip memory configuration.

IMPORTANT: When using tools, you need a memory of at least 3 messages to cover the tools interaction.

=== @MemoryId

In cases involving multiple users, ensure each user has a unique memory ID and pass this ID to the AI method:

[source,java]
----
String chat(@MemoryId int memoryId, @UserMessage String userMessage);
----

Also, remember to clear out users to prevent memory issues.

== Configuring Tools

Tools are methods that LLMs can invoke to access additional data. These methods, declared using the `@Tool` annotation, should be part of a bean:

[source,java]
----
@ApplicationScoped
public class CustomerRepository implements PanacheRepository<Customer> {

    @Tool("get the customer name for the given customerId")
    public String getCustomerName(long id) {
        return find("id", id).firstResult().name;
    }

}
----

The `@Tool` annotation can provide a description of the action, aiding the LLM in tool selection. The `@RegisterAiService` annotation allows configuring the tool provider:

[source,java]
----
@RegisterAiService(tools = {TransactionRepository.class, CustomerRepository.class })
----

IMPORTANT: Ensure you configure the memory provider when using tools.

IMPORTANT: Be cautious to avoid exposing destructive operations via tools.

// TODO: Add information about supported parameter types for tools.

More information about tools is available in the xref:./agent-and-tools.adoc[Agent and Tools] page.

== Configuring a Document Retriever

A document retriever fetches data from an external source and provides it to the LLM. It helps by sending only the relevant data, considering the LLM's context limitations.

// TODO: Provide detailed information about document retrievers.

This guidance aims to cover all crucial aspects of designing AI services with Quarkus, ensuring robust and efficient interactions with LLMs.

== Observability

Observability is built into services created via `@RegisterAiService` and is provided in the following form:

* Metrics are enabled when `quarkus-micrometer` is part of the application
* Traces are enabled when `quarkus-opentelemetry` is part of the application

=== Metrics

Each AI method is automatically timed and the timer data is available using the `langchain4j.aiservices.$interface_name.$method_name` template for the name.

For example, if the AI service looks like:

[source,java]
----
@RegisterAiService
public interface PoemAiService {

    @SystemMessage("You are a professional poet")
    @UserMessage("Write a poem about {topic}. The poem should be {lines} lines long")
    String writeAPoem(String topic, int lines);
}
----

and one chooses to use `quarkus-micrometer-registry-prometheus`, then the metrics could be:

[source]
----
# TYPE langchain4j_aiservices_MyAiService_writeAPoem_seconds_max gauge
# HELP langchain4j_aiservices_MyAiService_writeAPoem_seconds_max
langchain4j_aiservices_MyAiService_writeAPoem_seconds_max 19.791836635
# TYPE langchain4j_aiservices_MyAiService_writeAPoem_seconds summary
# HELP langchain4j_aiservices_MyAiService_writeAPoem_seconds
langchain4j_aiservices_MyAiService_writeAPoem_seconds_count 2.0
langchain4j_aiservices_MyAiService_writeAPoem_seconds_sum 29.992314839
----

=== Tracing

Each AI method creates its own span using the `langchain4j.aiservices.$interface_name.$method_name` template for the name.
Furthermore, tool invocations also create a span using `langchain4j.tools.$tool_name` template for the name.


For example, if the AI service looks like:

[source,java]
----
@RegisterAiService(tools = EmailService.class)
public interface PoemAiService {

    @SystemMessage("You are a professional poet")
    @UserMessage("Write a poem about {topic}. The poem should be {lines} lines long. Then send this poem by email.")
    String writeAPoem(String topic, int lines);

}
----

a tool that looks like:

[source,java]
----
@ApplicationScoped
public class EmailService {

    @Inject
    Mailer mailer;

    @Tool("send the given content by email")
    public void sendAnEmail(String content) {
        Log.info("Sending an email: " + content);
        mailer.send(Mail.withText("sendMeALetter@quarkus.io", "A poem for you", content));
    }

}
----

and invocation of the AI service that looks like:

[source,java]
----
@Path("/email-me-a-poem")
public class EmailMeAPoemResource {

    private final MyAiService service;

    public EmailMeAPoemResource(MyAiService service) {
        this.service = service;
    }

    @GET
    public String emailMeAPoem() {
        return service.writeAPoem("Quarkus", 4);
    }

}
----

then an example trace is:

image::trace.png[width=1000,align="center"]

In the trace above we can see the parent span which corresponds to the handling the GET HTTP request, but the real
interesting thing is the `langchain4j.aiservices.MyAiService.writeAPoem` span which corresponds to the invocation of the AI service.
The child spans of this span correspond (from to right) to calling the OpenAI API, invoking the `sendEmail` tool and finally invoking calling the OpenAI API again.

=== Auditing

The extension allows users to audit the process of implementing an AiService by introducing `io.quarkiverse.langchain4j.audit.AuditService` and `io.quarkiverse.langchain4j.audit.Audit`.
By default, if a bean of type `AuditService` is present in the application, it will be used in order to create an `Audit`, which received various callbacks pertaining to the implementation
of the AiService method. More information can be found on the javadoc of these two classes.
